{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip, pickle\n",
    "import sys\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.94792\n",
      "Valid accuracy: 0.9538\n",
      "Train accuracy: 0.95738\n",
      "Valid accuracy: 0.9605\n",
      "Train accuracy: 0.96268\n",
      "Valid accuracy: 0.9623\n"
     ]
    }
   ],
   "source": [
    "class ANN:\n",
    "    \n",
    "    \n",
    "    def __init__(self, train, valid, test, batch_size, epochs, learning_rate, nr_labels, neurons_per_layer: list, activation, \n",
    "                 optimizer, regularization, lambd, dropout):\n",
    "        \n",
    "        self.batch_size = batch_size; self.epochs = epochs; self.etha = learning_rate; self.nr_labels = nr_labels\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        \n",
    "        self.X_train = train[0]; self.Y_train = self.reshape_Y(train[1])\n",
    "        self.X_valid = valid[0]; self.Y_valid = self.reshape_Y(valid[1])\n",
    "        self.X_test = test[0];   self.Y_test = self.reshape_Y(test[1])\n",
    "        self.activation = activation; self.optimizer = optimizer.lower()\n",
    "        self.regularization = regularization; self.lambd = lambd\n",
    "        \n",
    "        if dropout is not None:\n",
    "            for key in dropout.keys():\n",
    "                dropout[key] = np.random.random_integers(low=0, high=self.neurons_per_layer[key]-1,\n",
    "                                                         size=(int)(dropout[key] * self.neurons_per_layer[key]))\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.batches = self.split_in_batches(self.X_train, self.Y_train, self.batch_size)\n",
    "        \n",
    "        self.a = [None for _ in range(0, len(self.neurons_per_layer))]\n",
    "        self.W = [np.random.normal(0, np.sqrt(1/self.neurons_per_layer[i]), (self.neurons_per_layer[i+1], self.neurons_per_layer[i]))\n",
    "                  for i in range(0, len(self.neurons_per_layer) - 1)]\n",
    "        self.W.insert(0, None)\n",
    "        \n",
    "        if self.optimizer == 'momentum':\n",
    "            self.gamma, self.history = self.auxiliar_vars('momentum')\n",
    "        elif self.optimizer == 'adagrad':\n",
    "            self.epsilon, self.history = self.auxiliar_vars('adagrad')\n",
    "        elif self.optimizer == 'rmsprop':\n",
    "            self.epsilon, self.beta, self.history = self.auxiliar_vars('rmsprop')\n",
    "    \n",
    "    \n",
    "    def auxiliar_vars(self, optimizer):\n",
    "        history = [0 for _ in range(0, len(self.W))]\n",
    "        epsilon = 0.01\n",
    "        if optimizer == 'momentum':\n",
    "            gamma = 0.9\n",
    "            return (gamma, history)\n",
    "        if optimizer == 'rmsprop':\n",
    "            beta = 0.99\n",
    "            return (epsilon, beta, history)\n",
    "        return (epsilon, history)\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def sigmoid_deriv(self, a):\n",
    "        return np.multiply(a, (1 - a))\n",
    "    \n",
    "    \n",
    "    def tanh(self, z):\n",
    "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def tanh_deriv(self, z):\n",
    "        return 1 - self.tanh(z) ** 2\n",
    "    \n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.array([list(map(lambda elem : 0 if elem <= 0 else elem, line)) for line in z])\n",
    "    \n",
    "    \n",
    "    def relu_deriv(self, a):\n",
    "        return np.array([list(map(lambda elem: 0 if elem == 0 else 1, line)) for line in a])\n",
    "    \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "    \n",
    "    \n",
    "    def softmax_deriv(self, a):\n",
    "        return np.multiply(a, 1 - a)\n",
    "    \n",
    "    \n",
    "    def logistic_loss(self, last_layer, real_outputs):\n",
    "        rez = 0 - np.multiply(real_outputs, np.log(last_layer)) + np.multiply((1 - real_outputs), np.log(1 - last_layer))\n",
    "        return sum(np.sum(rez, axis=0))\n",
    "    \n",
    "    \n",
    "    def logistic_loss_deriv(self, last_layer, real_outputs):\n",
    "        return (last_layer - real_outputs) / np.multiply(last_layer, 1 - last_layer)\n",
    "    \n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        self.a[0] = X\n",
    "        for i in range(1, len(self.a) - 1):\n",
    "            z = np.dot(self.W[i], self.a[i-1])\n",
    "            if self.activation == 'sigmoid':\n",
    "                self.a[i] = self.sigmoid(z)\n",
    "            elif self.activation == 'tanh':\n",
    "                self.a[i] = self.tanh(z)\n",
    "            elif self.activation == 'relu':\n",
    "                self.a[i] = self.relu(z)\n",
    "        i += 1\n",
    "        z = np.dot(self.W[i], self.a[i-1])\n",
    "        self.a[i] = self.softmax(z)\n",
    "\n",
    "        \n",
    "    def back_propagation(self, real_outputs):\n",
    "        W_before = copy.deepcopy(self.W)\n",
    "        gradients = []\n",
    "        error = (1 / self.batch_size) * (self.a[-1] - real_outputs)\n",
    "        if self.regularization == 'L2':\n",
    "            gradient = np.dot(error, self.a[-2].T) + self.lambd * (1 / self.batch_size) * self.W[-1]\n",
    "        elif self.regularization == 'L1':\n",
    "            gradient = np.dot(error, self.a[-2].T) + self.lambd * (1 / self.batch_size) * np.sign(self.W[-1])\n",
    "        gradients.append(gradient)\n",
    "        \n",
    "        for i in range(len(self.a) - 2, 0, -1):\n",
    "            if self.activation == 'sigmoid':\n",
    "                error = np.multiply(np.dot(error.T, self.W[i+1]).T, self.sigmoid_deriv(self.a[i]))\n",
    "            elif self.activation == 'tanh':\n",
    "                error = np.multiply(np.dot(error.T, self.W[i+1]).T, self.tanh_deriv(self.a[i]))\n",
    "            elif self.activation == 'relu':\n",
    "                error = np.multiply(np.dot(error.T, self.W[i+1]).T, self.relu_deriv(self.a[i]))\n",
    "            if self.regularization == 'L2':\n",
    "                gradient = np.dot(error, self.a[i-1].T) + self.lambd * (1 / self.batch_size) * self.W[i]\n",
    "            elif self.regularization == 'L1':\n",
    "                gradient = np.dot(error, self.a[i-1].T) + self.lambd * (1 / self.batch_size) * np.sign(self.W[i])\n",
    "            gradients.append(gradient)\n",
    "        \n",
    "        gradients = list(reversed(gradients))\n",
    "        gradients.insert(0, None)\n",
    "        \n",
    "        for i in range(1, len(self.a)):\n",
    "            if self.optimizer == 'momentum':\n",
    "                self.history[i] = self.gamma * self.history[i] + self.etha * gradients[i]\n",
    "                self.W[i] -= self.history[i]\n",
    "            else:\n",
    "                self.W[i] -=  (self.etha / np.sqrt(self.history[i] + self.epsilon)) * gradients[i]\n",
    "                if self.optimizer == 'adagrad':\n",
    "                    self.history[i] += gradients[i] ** 2\n",
    "                elif self.optimizer == 'rmsprop':\n",
    "                    self.history[i] = self.beta * self.history[i] + (1 - self.beta) * gradients[i] ** 2\n",
    "        \n",
    "        if self.dropout is not None:\n",
    "            for i in range(1, len(self.a) - 1):\n",
    "                if i in self.dropout.keys():\n",
    "                    for neuron in self.dropout[i]:\n",
    "                        self.W[i + 1][:, neuron] = copy.deepcopy(W_before[i + 1][:, neuron])\n",
    "                        self.W[i][neuron, :] = copy.deepcopy(W_before[i][neuron, :])\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        for epoch in range(0, self.epochs):\n",
    "            for batch in self.batches:\n",
    "                self.forward_propagation(batch[0])\n",
    "                self.back_propagation(batch[1])\n",
    "            self.accuracy(self.X_train, self.Y_train, \"Train accuracy: \")\n",
    "            self.accuracy(self.X_valid, self.Y_valid, \"Valid accuracy: \")\n",
    "    \n",
    "    \n",
    "    def accuracy(self, X, Y, msg):\n",
    "        self.forward_propagation(X)\n",
    "        real = np.argmax(Y, axis=0)\n",
    "        predicted = np.argmax(self.a[-1], axis=0)\n",
    "        nr = 0\n",
    "        for i in range(0, real.size):\n",
    "            if real[i] == predicted[i]:\n",
    "                nr += 1\n",
    "        print(msg + str(nr / X.shape[1]))\n",
    "    \n",
    "    \n",
    "    def reshape_Y(self, vec):    \n",
    "        new_Y = np.zeros((self.nr_labels, len(vec)))\n",
    "        for i in range(0, len(vec)):\n",
    "            new_Y[vec[i], i] = 1\n",
    "        return new_Y\n",
    "    \n",
    "    \n",
    "    def split_in_batches(self, X, Y, batch_size):\n",
    "        batches = []\n",
    "        for i in range(0, X.shape[1], batch_size):\n",
    "            batches.append((X[:, i:i+batch_size], Y[:, i:i+batch_size]))\n",
    "        return batches\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    f = gzip.open(r'C:\\Users\\Tudor\\Downloads\\mnist.pkl.gz', 'rb')\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    train, valid, test = u.load()\n",
    "    f.close()\n",
    "\n",
    "    var = ANN((train[0].T, train[1]), (valid[0].T, valid[1]), (test[0].T, test[1]), \n",
    "              batch_size=10, epochs=3, learning_rate=0.1, nr_labels=10, neurons_per_layer=[784,100,10], activation='relu',\n",
    "              optimizer='adagrad', regularization='L2', lambd=0.01, dropout=None)\n",
    "    var.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tomorrow', 'i', 'beg', 'you', 'call', '', 'now', 'today', 'hate', 'alywas', 'love']\n",
      "call\n",
      "[[0.0194261 ]\n",
      " [0.02425846]\n",
      " [0.22431047]\n",
      " [0.01155636]\n",
      " [0.303862  ]\n",
      " [0.02321062]\n",
      " [0.02357418]\n",
      " [0.02554114]\n",
      " [0.1783664 ]\n",
      " [0.01704568]\n",
      " [0.1488486 ]]\n",
      "you\n",
      "[[0.00306088]\n",
      " [0.00383662]\n",
      " [0.00206356]\n",
      " [0.97426685]\n",
      " [0.00259822]\n",
      " [0.00336771]\n",
      " [0.00325459]\n",
      " [0.00109777]\n",
      " [0.00187907]\n",
      " [0.00163794]\n",
      " [0.00293678]]\n",
      "alywas\n",
      "[[0.09942075]\n",
      " [0.01118215]\n",
      " [0.00845937]\n",
      " [0.00320009]\n",
      " [0.0094709 ]\n",
      " [0.00353116]\n",
      " [0.08172235]\n",
      " [0.25830025]\n",
      " [0.00797018]\n",
      " [0.50558486]\n",
      " [0.01115794]]\n",
      "\n",
      "[[1.35767337e-04]\n",
      " [1.46856917e-03]\n",
      " [1.03758827e-03]\n",
      " [4.02995365e-04]\n",
      " [1.32667765e-03]\n",
      " [9.90168893e-01]\n",
      " [9.63206430e-04]\n",
      " [1.44433448e-03]\n",
      " [1.33633194e-03]\n",
      " [5.16427063e-04]\n",
      " [1.19920968e-03]]\n"
     ]
    }
   ],
   "source": [
    "                 \n",
    "class RNN_M2M:\n",
    "    \n",
    "    def __init__(self, sentences, neurons_per_hl, nr_hls, epochs, learning_rate):    \n",
    "        self.sentences = sentences\n",
    "        self.preprocess_train()\n",
    "        self.neurons_per_hl = neurons_per_hl; self.nr_hls = nr_hls\n",
    "        self.epochs = epochs; self.etha = learning_rate\n",
    "        \n",
    "        self.s = [None for _ in range(0, nr_hls + 1)]\n",
    "        self.s[0] = np.zeros((neurons_per_hl, 1))\n",
    "        self.o = [None for _ in range(0, nr_hls + 1)]\n",
    "        \n",
    "        self.U = np.random.normal(0, np.sqrt(1 / self.X_train[0].shape[0]), (neurons_per_hl, len(self.vocab)))\n",
    "        self.W = np.random.normal(0, np.sqrt(1 / neurons_per_hl), (neurons_per_hl, neurons_per_hl))\n",
    "        self.V = np.random.normal(0, np.sqrt(1 / neurons_per_hl), (len(self.vocab), neurons_per_hl))\n",
    "        \n",
    "        \n",
    "    def preprocess_train(self):\n",
    "        self.vocab = set()\n",
    "        for sentence in self.sentences:\n",
    "            self.vocab.update(sentence)\n",
    "        self.vocab.add(\"\")\n",
    "        self.vocab = list(self.vocab)\n",
    "        print(self.vocab)\n",
    "\n",
    "        self.X_train = []\n",
    "        self.Y_train = []\n",
    "        for sentence in self.sentences:\n",
    "            x = []\n",
    "            for word in sentence:\n",
    "                one_hot = [0 for _ in range(0, len(self.vocab))]\n",
    "                one_hot[self.vocab.index(word)] = 1\n",
    "                x.append(one_hot)\n",
    "            self.X_train.append(np.array(x).T)\n",
    "            one_hot = [0 for _ in range(0, len(self.vocab))]\n",
    "            one_hot[self.vocab.index(\"\")] = 1\n",
    "            y = copy.deepcopy(x[1:])\n",
    "            y.append(one_hot)\n",
    "            self.Y_train.append(np.array(y).T)\n",
    "\n",
    "            \n",
    "    def tanh(self, z):\n",
    "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def tanh_deriv(self, a):\n",
    "        return 1 - np.multiply(a, a)\n",
    "    \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "    \n",
    "        \n",
    "    def forward_propagation(self, X):\n",
    "        for i in range(1, X.shape[1] + 1):\n",
    "            z = np.dot(self.W, self.s[i - 1]) + np.dot(self.U, X[:, i - 1].reshape((len(X[:, i - 1]), 1)))\n",
    "            self.s[i] = self.tanh(z)\n",
    "            z = np.dot(self.V, self.s[i])\n",
    "            self.o[i] = self.softmax(z)\n",
    "    \n",
    "    \n",
    "    def back_propagation_TT(self, X, Y):\n",
    "        gradient_U = 0; gradient_W = 0; gradient_V = 0\n",
    "        next_horizontal = 0\n",
    "        for i in range(len(self.o) - 1, 0, -1):\n",
    "            error = self.o[i] - Y[:, i - 1].reshape(len(Y[:, i - 1]), 1)\n",
    "            gradient_V += np.dot(error, self.s[i].T)\n",
    "            error = np.multiply(np.dot(error.T, self.V).T + next_horizontal, self.tanh_deriv(self.s[i]))\n",
    "            gradient_W += np.dot(error, self.s[i - 1].T)\n",
    "            gradient_U += np.dot(error, X[:, i - 1].reshape(1, len(X[:, i - 1])))\n",
    "            next_horizontal = np.dot(error.T, self.W).T\n",
    "        self.V -= self.etha * gradient_V\n",
    "        self.W -= self.etha * gradient_W\n",
    "        self.U -= self.etha * gradient_U\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        for epoch in range(0, self.epochs):\n",
    "            for i in range(0, len(self.X_train)):\n",
    "                self.forward_propagation(self.X_train[i])\n",
    "                self.back_propagation_TT(self.X_train[i], self.Y_train[i])\n",
    "\n",
    "\n",
    "    def test(self, word):\n",
    "        one_hot = [0 for _ in range(0, len(self.vocab))]\n",
    "        one_hot[self.vocab.index(word)] = 1\n",
    "        one_hot = np.array(one_hot).reshape((len(one_hot), 1))\n",
    "        for i in range(1, len(self.o)):\n",
    "            z = np.dot(self.W, self.s[i - 1]) + np.dot(self.U, one_hot)\n",
    "            self.s[i] = self.tanh(z)\n",
    "            z = np.dot(self.V, self.s[i])\n",
    "            self.o[i] = self.softmax(z)\n",
    "            one_hot = copy.deepcopy(self.o[i])\n",
    "    \n",
    "        \n",
    "        for i in range(1, len(self.o)):\n",
    "            print(self.vocab[np.argmax(self.o[i])])\n",
    "            print(self.o[i])\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    sentences = [[\"i\", \"love\", \"you\", \"now\"],\n",
    "                 [\"i\", \"hate\", \"you\", \"tomorrow\"],\n",
    "                 [\"i\", \"beg\", \"you\", \"today\"],\n",
    "                 [\"i\", \"call\", \"you\", \"alywas\"]]\n",
    "\n",
    "    var = RNN_M2M(sentences, 100, len(sentences[0]), 5, 0.1)\n",
    "    var.fit()\n",
    "    var.test('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h': 4, 'e': 3, 'l': 20, 'o': 60, ' ': 13}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "class RNN_M2M:\n",
    "    \n",
    "    def __init__(self, data, seq_length, neurons_in_hl, etha):\n",
    "        \n",
    "        self.data = data; self.vocab = list(set(data)); self.vocab_size = len(self.vocab); self.seq_length = seq_length\n",
    "        self.neurons_in_hl = neurons_in_hl; self.learning_rate = etha\n",
    "        \n",
    "        self.s = [None for i in range(0, self.seq_length + 1)]\n",
    "        self.s[0] = np.reshape(np.array([0 for i in range(0, self.neurons_in_hl)]), (self.neurons_in_hl, 1))\n",
    "        self.x = [np.array([None for i in range(0, self.vocab_size)]).reshape(self.vocab_size, 1) for j in range(0, self.seq_length)]\n",
    "        self.x.insert(0, None)\n",
    "        self.y = [np.array([None for i in range(0, self.vocab_size)]).reshape(self.vocab_size, 1) for j in range(0, self.seq_length)]\n",
    "        self.y.insert(0, None)\n",
    "        self.a = [np.array([0 for i in range(0, self.vocab_size)]).reshape(self.vocab_size, 1) for j in range(0, self.seq_length)]\n",
    "        self.a.insert(0, None)\n",
    "        \n",
    "        self.W = np.random.normal(0, np.sqrt(1/self.neurons_in_hl), (self.s[0].shape[0], self.s[0].shape[0]))\n",
    "        self.U = np.random.normal(0, np.sqrt(1/self.vocab_size), (self.s[0].shape[0], self.vocab_size))\n",
    "        self.V = np.random.normal(0, np.sqrt(1/self.neurons_in_hl), (self.vocab_size, self.s[0].shape[0]))\n",
    "        \n",
    "        self.input = []; self.target = []\n",
    "        for poz in range(0, len(self.data), self.seq_length):\n",
    "            if poz + self.seq_length + 1 < len(self.data):\n",
    "                self.input.append([self.vocab.index(self.data[i]) for i in range(poz, poz + self.seq_length)])\n",
    "                self.target.append([self.vocab.index(self.data[i]) for i in range(poz + 1, poz + self.seq_length + 1)])\n",
    "     \n",
    "    \n",
    "    def tanh(self, z):\n",
    "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def relu(self, z: np.array):\n",
    "        return np.array([list(map(lambda e: max(e, 0.01 * e), z[i])) for i in range(0, len(z))])\n",
    "    \n",
    "    \n",
    "    def deriv_relu(self, z):\n",
    "        return np.array([list(map(lambda e: 0 if e<0 else 1, elem)) for elem in z])\n",
    "    \n",
    "    \n",
    "    def deriv_tanh(self, s):\n",
    "        return 1 - np.multiply(self.tanh(s), self.tanh(s))\n",
    "    \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis = 0) \n",
    "    \n",
    "    \n",
    "    def BPTT(self, chars, target):\n",
    "        for i in range(0, len(chars)):\n",
    "            self.x[i + 1] = np.zeros((self.vocab_size, 1))\n",
    "            self.x[i + 1][chars[i]] = 1\n",
    "            self.y[i + 1] = np.zeros((self.vocab_size, 1))\n",
    "            self.y[i + 1][target[i]] = 1\n",
    "\n",
    "        for i in range(1, self.seq_length + 1):\n",
    "            z = np.dot(self.W, self.s[i-1]) + np.dot(self.U, self.x[i])\n",
    "            self.s[i] = self.relu(z)\n",
    "            z = np.dot(self.V, self.s[i])\n",
    "            self.a[i] = self.softmax(z)\n",
    "\n",
    "        gradient_W = np.zeros(self.W.shape)\n",
    "        gradient_U = np.zeros(self.U.shape)\n",
    "        gradient_V = np.zeros(self.V.shape)\n",
    "        gradientS_next = np.zeros(self.s[0].shape)\n",
    "\n",
    "        for i in range(self.seq_length, 0, -1):\n",
    "            error = self.a[i] - self.y[i]\n",
    "            gradient_V += np.dot(error, self.s[i].T)\n",
    "            new_error = np.multiply((np.dot(self.V.T, error) + gradientS_next), self.deriv_relu(self.s[i]))\n",
    "            gradient_W += np.dot(new_error, self.s[i-1].T)\n",
    "            gradient_U += np.dot(new_error, self.x[i].T)\n",
    "            gradientS_next = np.dot(self.W.T, new_error)\n",
    "            \n",
    "        for dparam in [gradient_V, gradient_W, gradient_U, ]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)              \n",
    "            \n",
    "        self.W -= self.learning_rate * gradient_W\n",
    "        self.V -= self.learning_rate * gradient_V\n",
    "        self.U -= self.learning_rate * gradient_U\n",
    "\n",
    "        self.s[0] = self.s[self.seq_length]\n",
    "\n",
    "    \n",
    "    def train_algo(self):\n",
    "        for i in range(0, len(self.input)):\n",
    "            self.BPTT(self.input[i], self.target[i])\n",
    "            \n",
    "    def test(self, text):\n",
    "        for i in range(0, len(text)):\n",
    "            self.x[i + 1] = np.zeros((self.vocab_size, 1))\n",
    "            self.x[i + 1][self.vocab.index(text[i])] = 1\n",
    "        for i in range(1, self.seq_length + 1):\n",
    "            z = np.dot(self.W, self.s[i-1]) + np.dot(self.U, self.x[i])\n",
    "            self.s[i] = self.relu(z)\n",
    "            z = np.dot(self.V, self.s[i])\n",
    "            self.a[i] = self.softmax(z)\n",
    "        #print(self.a[self.seq_length])\n",
    "        return self.vocab[int(np.argmax(self.a[-1], axis = 0))]\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "#     f = open(r\"C:\\Users\\Tudor\\Desktop\\DocForRNN.txt\", \"r\").read()\n",
    "    f = \"hello   hello   hello   hello   hello   hello   hello   hello   hello   hello   hello   hello   hello   hello   hello\"\n",
    "    d = {\"h\":0, \"e\":0, \"l\":0, \"o\": 0, \" \": 0}\n",
    "    for _ in range(100):\n",
    "        var = RNN_M2M(data=f, seq_length=4, neurons_in_hl=10, etha=0.05)\n",
    "        var.train_algo()\n",
    "        d[var.test(\"hell\")] += 1\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'him', 'miss', 'them', 'lose', 'think', 'envy', 'love', 'i', 'and', 'hate', 'trust', 'kill', 'her', '']\n",
      "after i comes: envy\n",
      "after envy comes: you\n",
      "after you comes: and\n",
      "after and comes: think\n",
      "after hate comes: them\n",
      "hate\n",
      "you\n",
      "and\n",
      "miss\n",
      "them\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RNN_M2M:\n",
    "    def __init__(self, vocab, neurons_per_hl, hls, learning_rate, epochs, X, Y):\n",
    "        self.vocab = vocab; self.neurons_per_hl = neurons_per_hl; self.hls = hls\n",
    "        self.etha = learning_rate; self.epochs = epochs; self.X = X; self.Y = Y\n",
    "        \n",
    "        self.o = [np.zeros((len(self.vocab), 1)) for _ in range(0, self.hls + 1)]\n",
    "        self.o[0] = None\n",
    "        \n",
    "        self.s = [np.zeros((self.neurons_per_hl, 1)) for _ in range(0, self.hls + 1)]\n",
    "        \n",
    "        self.W = np.random.normal(0, np.sqrt(1 / (self.neurons_per_hl - 1)), (self.neurons_per_hl, self.neurons_per_hl))\n",
    "        self.U = np.random.normal(0, np.sqrt(1 / (len(self.vocab) - 1)), (self.neurons_per_hl, len(self.vocab)))\n",
    "        self.V = np.random.normal(0, np.sqrt(1 / (self.neurons_per_hl - 1)), (len(self.vocab), self.neurons_per_hl))\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.array([list(map(lambda e: max(e, 0.01 * e), z[i])) for i in range(0, len(z))])\n",
    "    \n",
    "    def relu_deriv(self, z):\n",
    "        return np.array([list(map(lambda e: 0 if e < 0 else 1, elem)) for elem in z])\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis = 0) \n",
    "        \n",
    "    def forward_propagation(self, X):\n",
    "        for i in range(1, len(X)):\n",
    "            z1 = np.dot(self.U, X[i]) + np.dot(self.W, self.s[i - 1])\n",
    "            self.s[i] = self.relu(z1)\n",
    "            z2 = np.dot(self.V, self.s[i])\n",
    "            self.o[i] = self.softmax(z2)\n",
    "    \n",
    "    def back_propagation(self, X, Y):\n",
    "        gradient_W = 0; gradient_U = 0; gradient_V = 0\n",
    "        gradient_next = np.zeros((self.neurons_per_hl, 1))\n",
    "        for i in range(len(X) - 1, 0, -1):\n",
    "            error = (self.o[i] - Y[i])\n",
    "            gradient_V += np.dot(error, self.s[i].T)\n",
    "            next_error = np.multiply((np.dot(self.V.T, error) + gradient_next), self.relu_deriv(self.s[i]))\n",
    "            gradient_W += np.dot(next_error, self.s[i - 1].T)\n",
    "            gradient_U += np.dot(next_error, X[i].T)\n",
    "            gradient_next = np.dot(self.W.T, next_error) \n",
    "        \n",
    "        self.V -= self.etha * gradient_V\n",
    "        self.W -= self.etha * gradient_W\n",
    "        self.U -= self.etha * gradient_U  \n",
    "        \n",
    "#         self.s[0] = copy.deepcopy(self.s[-1])\n",
    "\n",
    "            \n",
    "    def fit(self):\n",
    "        for epoch in range(0, self.epochs):\n",
    "            for i in range(0, len(self.X)):\n",
    "                self.forward_propagation(self.X[i])\n",
    "                self.back_propagation(self.X[i], self.Y[i])\n",
    "        \n",
    "    def predict(self):\n",
    "        test = (\"i\", \"envy\", \"you\", \"and\", \"hate\")\n",
    "        x = []\n",
    "        x.insert(0, None)\n",
    "        for word in test:\n",
    "            one_hot_vec = np.zeros((len(self.vocab), 1))\n",
    "            one_hot_vec[self.vocab.index(word)] = 1\n",
    "            x.append(one_hot_vec)\n",
    "        #------------------------------\n",
    "        self.forward_propagation(x)\n",
    "        #------------------------------\n",
    "        for j in range(1, len(test) + 1):\n",
    "            aux_l =[]\n",
    "            for elem in self.o[j].tolist():\n",
    "                aux_l.extend(elem)\n",
    "            print(\"after\", test[j - 1], \"comes:\", self.vocab[np.random.choice([_ for _ in range(len(self.vocab))], p=aux_l)])\n",
    "        \n",
    "        \n",
    "        # Give the first word only, generate a sentence\n",
    "        test = \"i\"\n",
    "        one_hot_vec = np.zeros((len(self.vocab), 1))\n",
    "        one_hot_vec[self.vocab.index(test)] = 1\n",
    "        for j in range(1, len(self.X[0])):\n",
    "            z = np.dot(self.U, one_hot_vec) + np.dot(self.W, self.s[j - 1])\n",
    "            act = self.relu(z)\n",
    "            z = np.dot(self.V, act)\n",
    "            act = self.softmax(z)\n",
    "            aux_l = []\n",
    "            for elem in act:\n",
    "                aux_l.extend(elem)\n",
    "            print(self.vocab[np.random.choice([_ for _ in range(0, len(self.vocab))], p=aux_l)])\n",
    "            one_hot_vec = np.zeros((len(self.vocab), 1))\n",
    "            one_hot_vec[np.random.choice([_ for _ in range(0, len(self.vocab))], p=aux_l)] = 1\n",
    "\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    sentences = [(\"i\", \"love\", \"you\", \"and\", \"think\", \"her\"),\n",
    "                 (\"i\", \"hate\", \"you\", \"and\", \"miss\", \"her\"), \n",
    "                 (\"i\", \"lose\", \"you\", \"and\", \"trust\", \"him\"),\n",
    "                 (\"i\", \"envy\", \"you\", \"and\", \"hate\", \"him\"),\n",
    "                 (\"i\", \"kill\", \"you\", \"and\", \"miss\", \"them\"),\n",
    "                 (\"i\", \"trust\", \"you\", \"and\", \"think\", \"her\")]\n",
    "                \n",
    "    \n",
    "    vocab = set()\n",
    "    for sentence in sentences:\n",
    "        vocab.update(sentence)\n",
    "    vocab = list(vocab)\n",
    "    vocab.append(\"\")\n",
    "    print(vocab)\n",
    "          \n",
    "    X = []; Y = []\n",
    "    for sentence in sentences:\n",
    "        #----- construct X -----#\n",
    "        x = []\n",
    "        x.insert(0, None)\n",
    "        for word in sentence:\n",
    "            one_hot_vec = np.zeros((len(vocab), 1))\n",
    "            one_hot_vec[vocab.index(word)] = 1\n",
    "            x.append(one_hot_vec)\n",
    "        X.append(x)\n",
    "        #----- construct Y now -----#\n",
    "        y = []\n",
    "        y.insert(0, None)\n",
    "        for i in range(0, len(sentence) - 1):\n",
    "            one_hot_vec = np.zeros((len(vocab), 1))\n",
    "            one_hot_vec[vocab.index(sentence[i + 1])] = 1\n",
    "            y.append(one_hot_vec)\n",
    "        one_hot_vec = np.zeros((len(vocab), 1))\n",
    "        one_hot_vec[vocab.index(\"\")] = 1\n",
    "        y.append(one_hot_vec)\n",
    "        Y.append(y)\n",
    "        \n",
    "    model = RNN_M2M(vocab=vocab, neurons_per_hl=10, hls=len(sentences[0]), learning_rate=0.05, epochs=30, X=X, Y=Y)\n",
    "    model.fit()\n",
    "    model.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary is: ['beautiful', 'nice', 'love', 'ugly', 'hate']\n",
      "[[0.98538404]\n",
      " [0.01461596]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RNN_M2O:\n",
    "    \n",
    "    def __init__(self, nr_hls, neurons_per_layer, learning_rate, epochs, vocab, X, Y):\n",
    "        self.nr_hls = nr_hls; self.neurons_per_layer = neurons_per_layer; self.etha = learning_rate; self.epochs = epochs;\n",
    "        self.vocab = vocab; self.X = X; self.Y = Y\n",
    "        \n",
    "        self.s = [np.zeros((self.neurons_per_layer, 1)) for _ in range(0, self.nr_hls + 1)]\n",
    "        self.o = None\n",
    "        \n",
    "        self.W = np.random.normal(0, np.sqrt(1 / (self.neurons_per_layer - 1)), (self.neurons_per_layer, self.neurons_per_layer))\n",
    "        self.U = np.random.normal(0, np.sqrt(1 / (len(self.vocab) - 1)), (self.neurons_per_layer, len(self.vocab)))\n",
    "        self.V = np.random.normal(0, np.sqrt(1 / (self.neurons_per_layer - 1)), (2, self.neurons_per_layer))\n",
    "        \n",
    "    def relu(self, z):\n",
    "        return np.array([np.array(list(map(lambda e : max(0, e), line))) for line in z])\n",
    "    \n",
    "    def relu_deriv(self, z):\n",
    "        return  np.array([np.array(list(map(lambda e : 1 if e >=0 else 0.01 * e, line))) for line in z])\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        for i in range(1, len(X)):\n",
    "            z = np.dot(self.W, self.s[i - 1]) + np.dot(self.U, X[i])\n",
    "            self.s[i] = self.relu(z)\n",
    "        z = np.dot(self.V, self.s[i])\n",
    "        self.o = self.softmax(z)\n",
    "    \n",
    "    def back_propagation(self, X, Y):\n",
    "        error = self.o - Y\n",
    "        self.V -= self.etha * np.dot(error, self.s[-1].T)\n",
    "        \n",
    "        gradient_W = 0; gradient_U = 0\n",
    "        \n",
    "        error = np.multiply(np.dot(self.V.T, error), self.relu_deriv(self.s[-1]))\n",
    "        gradient_W += np.dot(error, self.s[-2].T)\n",
    "        gradient_U += np.dot(error, X[-1].T)\n",
    "\n",
    "        for i in range(len(X) - 2, 0, -1):\n",
    "            error = np.multiply(np.dot(self.W.T, error), self.relu_deriv(self.s[i]))\n",
    "            gradient_W += np.dot(error, self.s[i - 1].T)\n",
    "            gradient_U += np.dot(error, X[i].T)\n",
    "        \n",
    "        self.W -= self.etha * gradient_W\n",
    "        self.U -= self.etha * gradient_U\n",
    "        \n",
    "#         self.s[0] = copy.deepcopy(self.s[-1])\n",
    "        \n",
    "    def fit(self):\n",
    "        for epoch in range(0, self.epochs):\n",
    "            for i in range(0, len(self.X)):\n",
    "                self.forward_propagation(self.X[i])\n",
    "                self.back_propagation(self.X[i], self.Y[i])\n",
    "    \n",
    "    def predict(self):\n",
    "        sentence = (\"beautiful\", \"love\", \"nice\")\n",
    "        x = []\n",
    "        x.insert(0, None)\n",
    "        for word in sentence:\n",
    "            one_hot_vec = np.zeros((len(self.vocab), 1))\n",
    "            one_hot_vec[self.vocab.index(word)] = 1\n",
    "            x.append(one_hot_vec)\n",
    "                \n",
    "        self.forward_propagation(x)\n",
    "        print(self.o)\n",
    "                \n",
    "                \n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    sentences = [(\"beautiful\", \"nice\", \"love\"),\n",
    "                 (\"nice\", \"love\", \"nice\"),\n",
    "                 (\"love\", \"love\", \"love\"),\n",
    "                 (\"ugly\", \"hate\", \"ugly\"),\n",
    "                 (\"hate\", \"hate\", \"hate\")]\n",
    "    \n",
    "    Y = [np.array((1, 0)).reshape((2,1)), \n",
    "         np.array((1, 0)).reshape((2,1)), \n",
    "         np.array((1, 0)).reshape((2,1)),\n",
    "         np.array((0, 1)).reshape((2,1)), \n",
    "         np.array((0, 1)).reshape((2,1))]\n",
    "    \n",
    "    vocab = set()\n",
    "    for sentence in sentences:\n",
    "        vocab.update(sentence)\n",
    "    vocab = list(vocab)\n",
    "    print(\"Vocabulary is:\", vocab)\n",
    "          \n",
    "    X = []\n",
    "    for sentence in sentences:\n",
    "        #----- construct X -----#\n",
    "        x = []\n",
    "        x.insert(0, None)\n",
    "        for word in sentence:\n",
    "            one_hot_vec = np.zeros((len(vocab), 1))\n",
    "            one_hot_vec[vocab.index(word)] = 1\n",
    "            x.append(one_hot_vec)\n",
    "        X.append(x)\n",
    "    \n",
    "    obj = RNN_M2O(nr_hls=len(sentences[0]), neurons_per_layer=5, learning_rate=0.1, epochs=30, vocab=vocab, \n",
    "                  X=X, Y=Y)\n",
    "    obj.fit()\n",
    "    obj.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
